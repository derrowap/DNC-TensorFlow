Repeat-Copy is a task formulated in DeepMind's original DNC paper
\cite{graves2016hybrid} in order to test whether the architecture could
learn how to manage the external memory. The task itself isn't a difficult or
market demanding problem needing to be solved. The task simply has features in
which it can test the architecture of the DNC.

The Repeat-Copy task is described as follows. An agent receives as input
$n_{\textrm{min}}$ to $n_{\textrm{max}}$ sequences of $m$ many binary digits with
the task to repeat the sequences of binary digits a given $r_{\textrm{min}}$ to
$r_{\textrm{max}}$ many times. Consider an example when $n_{\textrm{min}} = 1$,
$n_{\textrm{max}} = 10$, $m = 4$, $r_{\textrm{min}} = 1$, and $r_{\textrm{max}} = 5$.
Then the agent would receive between 1 to 10 sequences of 4 binary digits with
the task of repeatedly outputing those sequences of 4 binary digits between 1
to 5 times. An equivalent task to this example would be to ask the agent to
output the following matrix 5 times:
$$
\begin{bmatrix}
    0 & 1 & 0 \\
    1 & 1 & 1 \\
    0 & 0 & 1 \\
    0 & 1 & 1
\end{bmatrix}
$$
where each of the $n$ many columns represent a sequence of $m$ binary digits.
The results reported by DeepMind's DNC \cite{graves2016hybrid} are on this
problem where $n_{\textrm{min}} = n_{\textrm{max}} = 5$, $m = 6$, and
$r_{\textrm{min}} = r_{\textrm{max}} = 1$.

The free gate in the DNC is active when the most recently read locations in
external memory can be freed. The allocation gate governs the strength to which
locations in external memory can be reused for new data. Training the DNC on
the Repeat-Copy task is forcing the model to learn when to free limited
external memory and reuse that freed memory to best take advantage of it.
Consider a task where the DNC receives a sequence of 10 random binary digits as
input and must repeat those 10 random binary digits as output. If the DNC uses
a feed-forward network with only an external memory of 10 locations, it must
learn to use the memory to only store data for each sequence at a time. The
DNC in this case doesn't have enough memory space to store all input it
receives to repeat. The results reported by DeepMind's DNC paper
\cite{graves2016hybrid} are such that the DNC learned how to manage external
memory as we would expect it must in order to succeed at the task. The free
gate became active while reading from external memory to immediately make that
space available for writing later. The allocation gate became active while
receiving input to write the input into the freed locations.

The purpose of repeating DeepMind's Repeat-Copy results \cite{graves2016hybrid}
in my implementation has two primary reasons. Firstly, to reconfirm the ability of
the DNC to learn how to manage memory efficiently. Secondly, to ensure my
implementation of the DNC behaves similar to that of DeepMind's before
attempting further tasks. My results are shown in Figure ~\ref{fig:repeatCopyResults}
and successfully repeat the same results reported by DeepMind's implementation
\cite{graves2016hybrid}.

\begin{figure}[h]
\includegraphics[width=0.4\textwidth]{../resources/RoseSeal.png}
\caption{My results on the Repeat-Copy task (image is a placeholder).}
\label{fig:repeatCopyResults}
\end{figure}
